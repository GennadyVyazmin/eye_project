# video_analytics_trassir_adjusted.py
import cv2
import numpy as np
import sqlite3
import datetime
import time
from deepface import DeepFace
import logging
import threading
from queue import Queue
from collections import deque, defaultdict
import os
import shutil

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ImprovedTrassirCounter:
    def __init__(self, processing_interval=1.0, similarity_threshold=0.65, tracking_threshold=0.50):
        """
        –í–µ—Ä—Å–∏—è —Å –æ—Å–ª–∞–±–ª–µ–Ω–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        self.conn = sqlite3.connect('visitors_trassir_improved.db', check_same_thread=False)
        self._init_database()

        self.processing_interval = processing_interval
        self.similarity_threshold = similarity_threshold
        self.tracking_threshold = tracking_threshold

        # –¶–≤–µ—Ç–∞ –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—É—Å–æ–≤
        self.COLORS = {
            'detected': (0, 255, 0),  # –ó–µ–ª–µ–Ω—ã–π - –ª–∏—Ü–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ
            'tracking': (255, 255, 0),  # –ñ–µ–ª—Ç—ã–π - —Å–æ–∑–¥–∞–Ω —Ç—Ä–µ–∫
            'known': (0, 255, 255),  # –ì–æ–ª—É–±–æ–π - –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
            'new': (0, 0, 255),  # –ö—Ä–∞—Å–Ω—ã–π - –Ω–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤ –ë–î
            'analyzing': (255, 165, 0),  # –û—Ä–∞–Ω–∂–µ–≤—ã–π - –∞–Ω–∞–ª–∏–∑ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ
            'rejected': (128, 128, 128)  # –°–µ—Ä—ã–π - –æ—Ç—Å–µ—è–Ω –ª–æ–∂–Ω—ã–π –æ–±—ä–µ–∫—Ç
        }

        # –ü–∞–ø–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ
        self.photos_dir = "visitor_photos_improved"
        self.current_session_dir = "current_session"
        self._create_directories()

        # –¢—Ä–µ–∫–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.last_processing_time = 0
        self.known_visitors_cache = {}
        self.frame_count = 0

        # –°–∏—Å—Ç–µ–º–∞ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ª–∏—Ü
        self.face_tracks = {}
        self.next_track_id = 1
        self.track_max_age = 8.0

        # –ì–∞–ª–µ—Ä–µ—è —Ç–µ–∫—É—â–∏—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π —Å –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–æ–π
        self.current_visitors_gallery = {}
        self.gallery_max_size = 8
        self.gallery_cleanup_interval = 60.0
        self.last_gallery_cleanup = time.time()
        self.photo_size = (120, 160)

        # –û–°–õ–ê–ë–õ–ï–ù–ù–´–ï —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.false_positive_filter = {
            'min_face_ratio': 0.02,  # –°–ù–ò–ñ–ï–ù–û: –±—ã–ª–æ 0.08
            'max_face_ratio': 0.60,  # –ü–û–í–´–®–ï–ù–û: –±—ã–ª–æ 0.40
            'min_aspect_ratio': 0.5,  # –°–ù–ò–ñ–ï–ù–û: –±—ã–ª–æ 0.7
            'max_aspect_ratio': 2.0,  # –ü–û–í–´–®–ï–ù–û: –±—ã–ª–æ 1.4
            'min_brightness': 20,  # –°–ù–ò–ñ–ï–ù–û: –±—ã–ª–æ 30
            'max_brightness': 240,  # –ü–û–í–´–®–ï–ù–û: –±—ã–ª–æ 220
            'edge_threshold': 20,  # –°–ù–ò–ñ–ï–ù–û: –±—ã–ª–æ 50
            'required_confirmations': 2  # –°–ù–ò–ñ–ï–ù–û: –±—ã–ª–æ 3
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.recognition_stats = {
            'total_detections': 0,
            'valid_detections': 0,
            'rejected_detections': 0,
            'new_visitors': 0,
            'known_visitors': 0,
            'frames_processed': 0,
            'quality_rejections': defaultdict(int)
        }
        self.last_log_time = time.time()

        # –û—á–µ—Ä–µ–¥—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.frame_queue = Queue(maxsize=1)
        self.results_queue = Queue()

        # –î–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )

        self.alt_face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml'
        )

        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π
        self._load_known_visitors()

        # –ü–æ—Ç–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.processing_thread = None
        self.stop_processing = False

        # –î–ª—è —Ä–∞—Å—á–µ—Ç–∞ FPS
        self.fps_start_time = time.time()
        self.fps_frame_count = 0
        self.current_fps = 0

        logger.info("üéØ –°–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å –û–°–õ–ê–ë–õ–ï–ù–ù–´–ú–ò —Ñ–∏–ª—å—Ç—Ä–∞–º–∏")

    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ"""
        os.makedirs(self.photos_dir, exist_ok=True)
        os.makedirs(os.path.join(self.photos_dir, self.current_session_dir), exist_ok=True)
        logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω—ã –ø–∞–ø–∫–∏ –¥–ª—è —Ñ–æ—Ç–æ: {self.photos_dir}")

    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS visitors (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                face_embedding BLOB,
                first_seen TIMESTAMP,
                last_seen TIMESTAMP,
                visit_count INTEGER DEFAULT 1,
                last_updated TIMESTAMP,
                confirmed_count INTEGER DEFAULT 1,
                photo_path TEXT,
                quality_score REAL DEFAULT 1.0
            )
        ''')
        self.conn.commit()

    def _load_known_visitors(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT id, face_embedding, photo_path FROM visitors")
        visitors = cursor.fetchall()

        self.known_visitors_cache.clear()
        for visitor_id, embedding_blob, photo_path in visitors:
            if embedding_blob:
                try:
                    embedding = np.frombuffer(embedding_blob, dtype=np.float32)
                    self.known_visitors_cache[visitor_id] = embedding
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è {visitor_id}: {e}")

        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π: {len(self.known_visitors_cache)}")

    def setup_rtsp_camera(self, rtsp_url):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ RTSP –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"""
        logger.info(f"üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–∞–º–µ—Ä–µ: {rtsp_url}")
        cap = cv2.VideoCapture(rtsp_url)

        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        cap.set(cv2.CAP_PROP_FPS, 15)
        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'H264'))

        for _ in range(10):
            cap.read()

        if cap.isOpened():
            ret, test_frame = cap.read()
            if ret:
                logger.info(f"‚úÖ –ö–∞–º–µ—Ä–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞. –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {test_frame.shape[1]}x{test_frame.shape[0]}")
            else:
                logger.error("‚ùå –ö–∞–º–µ—Ä–∞ –Ω–µ –ø–µ—Ä–µ–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∫–∞–º–µ—Ä–µ")

        return cap

    def analyze_face_quality(self, face_image, bbox, frame_size):
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –ª–∏—Ü–∞ —Å –æ—Ç–ª–∞–¥–∫–æ–π"""
        try:
            x, y, w, h = bbox
            frame_height, frame_width = frame_size

            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –ª–∏—Ü–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–∞–¥—Ä–∞
            face_area = w * h
            frame_area = frame_width * frame_height
            face_ratio = face_area / frame_area

            logger.debug(f"üìè –†–∞–∑–º–µ—Ä –ª–∏—Ü–∞: {w}x{h}, –æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {face_ratio:.4f}")

            if face_ratio < self.false_positive_filter['min_face_ratio']:
                self.recognition_stats['quality_rejections']['small_size'] += 1
                return False, f"–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ –ª–∏—Ü–æ ({face_ratio:.4f} < {self.false_positive_filter['min_face_ratio']})"
            if face_ratio > self.false_positive_filter['max_face_ratio']:
                self.recognition_stats['quality_rejections']['large_size'] += 1
                return False, f"–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ –ª–∏—Ü–æ ({face_ratio:.4f} > {self.false_positive_filter['max_face_ratio']})"

            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω
            aspect_ratio = w / h
            logger.debug(f"‚öñÔ∏è –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω: {aspect_ratio:.2f}")

            if aspect_ratio < self.false_positive_filter['min_aspect_ratio']:
                self.recognition_stats['quality_rejections']['narrow'] += 1
                return False, f"–°–ª–∏—à–∫–æ–º —É–∑–∫–æ–µ ({aspect_ratio:.2f} < {self.false_positive_filter['min_aspect_ratio']})"
            if aspect_ratio > self.false_positive_filter['max_aspect_ratio']:
                self.recognition_stats['quality_rejections']['wide'] += 1
                return False, f"–°–ª–∏—à–∫–æ–º —à–∏—Ä–æ–∫–æ–µ ({aspect_ratio:.2f} > {self.false_positive_filter['max_aspect_ratio']})"

            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —è—Ä–∫–æ—Å—Ç–∏
            gray_face = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
            brightness = np.mean(gray_face)
            logger.debug(f"üí° –Ø—Ä–∫–æ—Å—Ç—å: {brightness:.1f}")

            if brightness < self.false_positive_filter['min_brightness']:
                self.recognition_stats['quality_rejections']['dark'] += 1
                return False, f"–°–ª–∏—à–∫–æ–º —Ç–µ–º–Ω–æ–µ ({brightness:.1f} < {self.false_positive_filter['min_brightness']})"
            if brightness > self.false_positive_filter['max_brightness']:
                self.recognition_stats['quality_rejections']['bright'] += 1
                return False, f"–°–ª–∏—à–∫–æ–º —Å–≤–µ—Ç–ª–æ–µ ({brightness:.1f} > {self.false_positive_filter['max_brightness']})"

            # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ç–∫–æ—Å—Ç–∏ (–ª–∞–ø–ª–∞—Å–∏–∞–Ω)
            laplacian_var = cv2.Laplacian(gray_face, cv2.CV_64F).var()
            logger.debug(f"üîç –ß–µ—Ç–∫–æ—Å—Ç—å (–ª–∞–ø–ª–∞—Å–∏–∞–Ω): {laplacian_var:.1f}")

            if laplacian_var < self.false_positive_filter['edge_threshold']:
                self.recognition_stats['quality_rejections']['blurry'] += 1
                return False, f"–ù–µ—á–µ—Ç–∫–æ–µ ({laplacian_var:.1f} < {self.false_positive_filter['edge_threshold']})"

            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ –æ–±–ª–∞—Å—Ç–∏
            contrast = np.std(gray_face)
            logger.debug(f"üé® –ö–æ–Ω—Ç—Ä–∞—Å—Ç: {contrast:.1f}")

            if contrast < 5:  # –ï—â–µ –±–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥
                self.recognition_stats['quality_rejections']['uniform'] += 1
                return False, f"–°–ª–∏—à–∫–æ–º –æ–¥–Ω–æ—Ä–æ–¥–Ω–∞—è –æ–±–ª–∞—Å—Ç—å ({contrast:.1f})"

            return True, f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ OK (—è—Ä–∫–æ—Å—Ç—å: {brightness:.1f}, —á–µ—Ç–∫–æ—Å—Ç—å: {laplacian_var:.1f}, –∫–æ–Ω—Ç—Ä–∞—Å—Ç: {contrast:.1f})"

        except Exception as e:
            self.recognition_stats['quality_rejections']['error'] += 1
            return False, f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}"

    def detect_faces_robust(self, frame):
        """–ù–∞–¥–µ–∂–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü —Å –û–°–õ–ê–ë–õ–ï–ù–ù–û–ô —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        frame_size = gray.shape

        all_faces = []

        # –ü–ï–†–í–´–ô –ü–†–û–•–û–î: –î–µ—Ç–µ–∫—Ü–∏—è –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        faces1 = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            minNeighbors=4,  # –ú–µ–Ω—å—à–µ —Å–æ—Å–µ–¥–µ–π –¥–ª—è –±–æ–ª—å—à–µ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            minSize=(30, 30),  # –ú–µ–Ω—å—à–∏–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            maxSize=(400, 400),
            flags=cv2.CASCADE_SCALE_IMAGE
        )

        faces2 = self.alt_face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=3,  # –ï—â–µ –º–µ–Ω—å—à–µ —Å–æ—Å–µ–¥–µ–π
            minSize=(25, 25),  # –ï—â–µ –º–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä
            maxSize=(500, 500),
            flags=cv2.CASCADE_SCALE_IMAGE
        )

        logger.info(f"üîç –°–´–†–ê–Ø –¥–µ—Ç–µ–∫—Ü–∏—è: –æ—Å–Ω–æ–≤–Ω–æ–π {len(faces1)}, –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π {len(faces2)}")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        face_set = set()
        valid_count = 0
        rejected_count = 0

        for faces in [faces1, faces2]:
            for (x, y, w, h) in faces:
                # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –±–ª–∏–∑–∫–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
                face_key = (x // 10, y // 10, w // 10, h // 10)  # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
                if face_key in face_set:
                    continue

                face_set.add(face_key)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞
                face_roi = frame[y:y + h, x:x + w]
                is_valid, quality_msg = self.analyze_face_quality(face_roi, (x, y, w, h), frame_size)

                if is_valid:
                    all_faces.append((x, y, w, h))
                    valid_count += 1
                    logger.info(f"‚úÖ –ü—Ä–∏–Ω—è—Ç–æ –ª–∏—Ü–æ {w}x{h}: {quality_msg}")
                else:
                    rejected_count += 1
                    logger.info(f"‚ùå –û—Ç–∫–ª–æ–Ω–µ–Ω–æ {w}x{h}: {quality_msg}")

        logger.info(f"üìä –ò–¢–û–ì–û: –ø—Ä–∏–Ω—è—Ç–æ {valid_count}, –æ—Ç–∫–ª–æ–Ω–µ–Ω–æ {rejected_count}")
        self.recognition_stats['rejected_detections'] += rejected_count

        return all_faces

    def get_fast_embedding(self, face_image):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å –£–ü–†–û–©–ï–ù–ù–û–ô –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        try:
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            face_resized = cv2.resize(face_image, (160, 160))
            face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            result = DeepFace.represent(
                face_rgb,
                model_name='Facenet',
                enforce_detection=False,
                detector_backend='opencv',
                align=False  # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            )

            embedding = np.array(result[0]['embedding'], dtype=np.float32)

            # –û–ß–ï–ù–¨ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            if np.all(embedding == 0):
                logger.warning("‚ùå –ù—É–ª–µ–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥")
                return None

            norm = np.linalg.norm(embedding)
            if norm < 0.01:  # –û—á–µ–Ω—å –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥
                logger.warning(f"‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è –Ω–æ—Ä–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {norm}")
                return None

            logger.debug(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–ª—É—á–µ–Ω, –Ω–æ—Ä–º–∞: {norm:.4f}")
            return embedding

        except Exception as e:
            logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None

    def calculate_similarity(self, embedding1, embedding2):
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –ª–∏—Ü"""
        if embedding1 is None or embedding2 is None:
            return 0.0

        try:
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            emb1_norm = embedding1 / norm1
            emb2_norm = embedding2 / norm2

            similarity = float(np.dot(emb1_norm, emb2_norm))
            logger.debug(f"üìê –°—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f}")
            return max(0.0, min(1.0, similarity))

        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏: {e}")
            return 0.0

    def find_best_match(self, embedding):
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        if embedding is None:
            return None, 0.0

        best_match_id = None
        best_similarity = 0.0

        for visitor_id, known_embedding in self.known_visitors_cache.items():
            similarity = self.calculate_similarity(embedding, known_embedding)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match_id = visitor_id

        logger.debug(f"üîç –õ—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: ID {best_match_id}, —Å—Ö–æ–∂–µ—Å—Ç—å {best_similarity:.3f}")
        return best_match_id, best_similarity

    def cleanup_old_gallery_entries(self):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –≥–∞–ª–µ—Ä–µ–∏"""
        current_time = time.time()
        if current_time - self.last_gallery_cleanup >= self.gallery_cleanup_interval:
            removed_count = 0
            for visitor_id in list(self.current_visitors_gallery.keys()):
                last_seen = self.current_visitors_gallery[visitor_id]['last_seen']
                if current_time - last_seen > self.gallery_cleanup_interval:
                    del self.current_visitors_gallery[visitor_id]
                    removed_count += 1

            if removed_count > 0:
                logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –≥–∞–ª–µ—Ä–µ–∏: —É–¥–∞–ª–µ–Ω–æ {removed_count} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π")

            self.last_gallery_cleanup = current_time

    def update_face_tracking(self, current_faces, timestamp):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ª–∏—Ü –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏"""
        updated_faces = []

        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Ç—Ä–µ–∫–∏
        for track_id in list(self.face_tracks.keys()):
            if timestamp - self.face_tracks[track_id]['last_seen'] > self.track_max_age:
                logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ç—Ä–µ–∫ {track_id}")
                del self.face_tracks[track_id]

        for face_data in current_faces:
            embedding = face_data['embedding']
            coords = face_data['coords']
            face_image = face_data['face_image']

            best_track_id = None
            best_similarity = 0.0

            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ç—Ä–µ–∫–∞–º–∏
            for track_id, track_data in self.face_tracks.items():
                similarity = self.calculate_similarity(embedding, track_data['embedding'])
                if similarity > best_similarity and similarity > self.tracking_threshold:
                    best_similarity = similarity
                    best_track_id = track_id

            if best_track_id is not None:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–µ–∫
                track_data = self.face_tracks[best_track_id]
                track_data.update({
                    'embedding': embedding,
                    'last_seen': timestamp,
                    'coords': coords,
                    'face_image': face_image,
                    'confirmation_count': track_data.get('confirmation_count', 0) + 1
                })
                face_data['track_id'] = best_track_id
                face_data['visitor_id'] = track_data.get('visitor_id')
                face_data['status'] = 'tracking'
                logger.debug(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω —Ç—Ä–µ–∫ {best_track_id}, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π: {track_data['confirmation_count']}")
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç—Ä–µ–∫
                track_id = self.next_track_id
                self.next_track_id += 1
                self.face_tracks[track_id] = {
                    'embedding': embedding,
                    'last_seen': timestamp,
                    'coords': coords,
                    'face_image': face_image,
                    'visitor_id': None,
                    'created_at': timestamp,
                    'confirmation_count': 1
                }
                face_data['track_id'] = track_id
                face_data['status'] = 'tracking'
                logger.info(f"üéØ –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —Ç—Ä–µ–∫ {track_id}")

            updated_faces.append(face_data)

        return updated_faces

    def confirm_visitor_identity(self, track_id, face_data):
        """–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è —Å –£–ü–†–û–©–ï–ù–ù–û–ô –ª–æ–≥–∏–∫–æ–π"""
        track_data = self.face_tracks.get(track_id)
        if not track_data:
            return None

        embedding = face_data['embedding']
        face_image = face_data['face_image']
        timestamp = time.time()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        confirmation_count = track_data.get('confirmation_count', 0)
        min_confirmations = self.false_positive_filter['required_confirmations']

        if confirmation_count < min_confirmations:
            face_data['status'] = 'analyzing'
            logger.debug(f"‚è≥ –¢—Ä–µ–∫ {track_id} –æ–∂–∏–¥–∞–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π: {confirmation_count}/{min_confirmations}")
            return None

        # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å visitor_id, –æ–±–Ω–æ–≤–ª—è–µ–º –µ–≥–æ
        if track_data['visitor_id']:
            visitor_id = track_data['visitor_id']
            self.recognition_stats['known_visitors'] += 1
            face_data['status'] = 'known'
            self.update_visitor_gallery(visitor_id, face_image, 'known')
            logger.debug(f"‚ôªÔ∏è  –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—å {visitor_id}")
            return visitor_id

        # –ò—â–µ–º –ª—É—á—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å—Ä–µ–¥–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö
        visitor_id, similarity = self.find_best_match(embedding)

        if similarity > self.similarity_threshold:
            # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è
            track_data['visitor_id'] = visitor_id
            track_data['confirmed_at'] = timestamp
            self.recognition_stats['known_visitors'] += 1
            face_data['status'] = 'known'
            self.update_visitor_gallery(visitor_id, face_image, 'known')
            logger.info(f"üë§ –û–ü–û–ó–ù–ê–ù –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—å {visitor_id}, —Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f}")
            return visitor_id
        else:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è
            track_duration = timestamp - track_data['created_at']
            if track_duration > 2.0:  # –£–º–µ–Ω—å—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
                new_visitor_id = self._create_new_visitor(embedding, face_image, track_id)
                if new_visitor_id:
                    self.recognition_stats['new_visitors'] += 1
                    face_data['status'] = 'new'
                    self.update_visitor_gallery(new_visitor_id, face_image, 'new')
                    logger.info(f"üÜï –°–û–ó–î–ê–ù –Ω–æ–≤—ã–π –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—å {new_visitor_id}, —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏: {similarity:.3f}")
                return new_visitor_id

        return None

    def _create_new_visitor(self, embedding, face_image, track_id):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è"""
        cursor = self.conn.cursor()
        now = datetime.datetime.now()

        visitor_id = None
        try:
            embedding_blob = embedding.astype(np.float32).tobytes()

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ñ–æ—Ç–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è ID
            photo_path = self.save_visitor_photo(face_image, "temp")

            cursor.execute(
                """INSERT INTO visitors (face_embedding, first_seen, last_seen, 
                   visit_count, last_updated, confirmed_count, photo_path, quality_score) 
                   VALUES (?, ?, ?, 1, ?, 1, ?, 1.0)""",
                (embedding_blob, now, now, now, photo_path)
            )
            visitor_id = cursor.lastrowid

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–æ—Ç–æ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º ID
            final_photo_path = self.save_visitor_photo(face_image, visitor_id)
            cursor.execute(
                "UPDATE visitors SET photo_path = ? WHERE id = ?",
                (final_photo_path, visitor_id)
            )

            self.known_visitors_cache[visitor_id] = embedding
            self.conn.commit()

            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ñ–æ—Ç–æ
            if os.path.exists(photo_path):
                os.remove(photo_path)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è: {e}")
            self.conn.rollback()
            return None

        if track_id in self.face_tracks:
            self.face_tracks[track_id]['visitor_id'] = visitor_id

        return visitor_id

    def save_visitor_photo(self, face_image, visitor_id):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–æ—Ç–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è"""
        try:
            photo_clean = face_image.copy()

            height, width = photo_clean.shape[:2]
            if width < 100:  # –£–º–µ–Ω—å—à–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                scale = 100 / width
                new_width = 100
                new_height = int(height * scale)
                photo_clean = cv2.resize(photo_clean, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)

            filename = f"visitor_{visitor_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
            filepath = os.path.join(self.photos_dir, filename)
            session_filepath = os.path.join(self.photos_dir, self.current_session_dir, filename)

            cv2.imwrite(filepath, photo_clean)
            cv2.imwrite(session_filepath, photo_clean)

            return filepath

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ: {e}")
            return ""

    def update_visitor_gallery(self, visitor_id, face_image, status):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥–∞–ª–µ—Ä–µ–∏ —Ç–µ–∫—É—â–∏—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π"""
        try:
            gallery_photo = face_image.copy()

            border_color = self.COLORS.get(status, (255, 255, 255))
            gallery_photo = cv2.copyMakeBorder(
                gallery_photo, 5, 25, 5, 5, cv2.BORDER_CONSTANT, value=border_color
            )

            status_text = self.get_status_text(status)
            cv2.putText(gallery_photo, f"ID: {visitor_id}", (10, gallery_photo.shape[0] - 15),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, border_color, 1)

            gallery_photo = cv2.resize(gallery_photo, self.photo_size, interpolation=cv2.INTER_AREA)

            self.current_visitors_gallery[visitor_id] = {
                'photo': gallery_photo,
                'last_seen': time.time(),
                'status': status
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–∞–ª–µ—Ä–µ–∏: {e}")

    def get_status_text(self, status):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ç–∞—Ç—É—Å—É"""
        status_texts = {
            'detected': 'DETECTED',
            'tracking': 'TRACKING',
            'analyzing': 'ANALYZING',
            'known': 'KNOWN',
            'new': 'NEW USER',
            'rejected': 'REJECTED'
        }
        return status_texts.get(status, 'UNKNOWN')

    def get_color_by_status(self, status):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –ø–æ —Å—Ç–∞—Ç—É—Å—É"""
        return self.COLORS.get(status, (255, 255, 255))

    def create_gallery_display(self, main_frame):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥–∞–ª–µ—Ä–µ–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π —Å –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–æ–π"""
        try:
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
            self.cleanup_old_gallery_entries()

            main_height, main_width = main_frame.shape[:2]
            gallery_width = 300
            gallery_panel = np.zeros((main_height, gallery_width, 3), dtype=np.uint8)

            cv2.putText(gallery_panel, "CURRENT VISITORS", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(gallery_panel, f"Active: {len(self.current_visitors_gallery)}", (10, 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            if self.current_visitors_gallery:
                visitor_ids = sorted(self.current_visitors_gallery.keys())
                photos_per_column = 4
                photo_width, photo_height = self.photo_size
                margin = 10

                for i, visitor_id in enumerate(visitor_ids):
                    if i >= self.gallery_max_size:
                        break

                    visitor_data = self.current_visitors_gallery[visitor_id]
                    row = i % photos_per_column
                    col = i // photos_per_column

                    x = margin + col * (photo_width + margin)
                    y = 80 + row * (photo_height + margin)

                    if y + photo_height < main_height and x + photo_width < gallery_width:
                        gallery_panel[y:y + photo_height, x:x + photo_width] = visitor_data['photo']

                        status_color = self.COLORS.get(visitor_data['status'], (255, 255, 255))
                        cv2.circle(gallery_panel, (x + 10, y + 10), 5, status_color, -1)
            else:
                cv2.putText(gallery_panel, "No active", (50, main_height // 2),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 1)
                cv2.putText(gallery_panel, "visitors", (60, main_height // 2 + 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 1)

            combined_frame = np.hstack([main_frame, gallery_panel])
            return combined_frame

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥–∞–ª–µ—Ä–µ–∏: {e}")
            return main_frame

    def resize_frame_for_display(self, frame, target_width=1280):
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–∞–¥—Ä–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        height, width = frame.shape[:2]

        if width <= target_width:
            return frame

        ratio = target_width / width
        new_width = target_width
        new_height = int(height * ratio)

        resized_frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_LINEAR)

        return resized_frame

    def log_recognition_stats(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è"""
        current_time = time.time()
        if current_time - self.last_log_time >= 5.0:
            logger.info(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê: –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(self.known_visitors_cache)}, "
                        f"–ê–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤: {len(self.face_tracks)}, "
                        f"–í –≥–∞–ª–µ—Ä–µ–µ: {len(self.current_visitors_gallery)}, "
                        f"–ù–æ–≤—ã—Ö –∑–∞ —Å–µ—Å—Å–∏—é: {self.recognition_stats['new_visitors']}, "
                        f"–ò–∑–≤–µ—Å—Ç–Ω—ã—Ö: {self.recognition_stats['known_visitors']}, "
                        f"–û—Ç–∫–ª–æ–Ω–µ–Ω–æ: {self.recognition_stats['rejected_detections']}")

            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏—á–∏–Ω–∞–º –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            if self.recognition_stats['quality_rejections']:
                logger.info(f"üìã –ü–†–ò–ß–ò–ù–´ –û–¢–ö–õ–û–ù–ï–ù–ò–Ø: {dict(self.recognition_stats['quality_rejections'])}")

            self.last_log_time = current_time

    def start_processing_thread(self):
        """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.stop_processing = False
        self.processing_thread = threading.Thread(target=self._processing_worker, daemon=True)
        self.processing_thread.start()
        logger.info("–§–æ–Ω–æ–≤—ã–π –ø–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—É—â–µ–Ω")

    def _processing_worker(self):
        """–§–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–æ–≤"""
        while not self.stop_processing:
            try:
                frame_data = self.frame_queue.get(timeout=1.0)
                frame, frame_time = frame_data

                result = self._process_frame_heavy(frame)
                self.results_queue.put((result, frame_time))

                self.frame_queue.task_done()

            except:
                continue

    def _process_frame_heavy(self, frame):
        """–¢—è–∂–µ–ª—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –û–°–õ–ê–ë–õ–ï–ù–ù–û–ô —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        try:
            # –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
            faces = self.detect_faces_robust(frame)

            processed_faces = []
            if len(faces) > 0:
                self.recognition_stats['total_detections'] += len(faces)
                self.recognition_stats['valid_detections'] += len(faces)
                logger.info(f"üë• –û–ë–ù–ê–†–£–ñ–ï–ù–û –õ–ò–¶: {len(faces)}")

                for (x, y, w, h) in faces:
                    face_img = frame[y:y + h, x:x + w]

                    embedding = self.get_fast_embedding(face_img)
                    if embedding is not None:
                        visitor_id, similarity = self.find_best_match(embedding)

                        processed_faces.append({
                            'coords': (x, y, w, h),
                            'embedding': embedding,
                            'similarity': similarity,
                            'visitor_id': visitor_id,
                            'status': 'detected',
                            'face_image': face_img
                        })
                    else:
                        logger.debug("‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ –ª–∏—Ü–æ –∏–∑-–∑–∞ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞")

            return {
                'faces': processed_faces,
                'processed_count': len(processed_faces),
                'detected_count': len(faces),
                'timestamp': time.time()
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            return {'faces': [], 'processed_count': 0, 'detected_count': 0}

    def process_frame_realtime(self, frame):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
        current_time = time.time()

        # –û–±–Ω–æ–≤–ª—è–µ–º FPS
        self.fps_frame_count += 1
        if current_time - self.fps_start_time >= 1.0:
            self.current_fps = self.fps_frame_count / (current_time - self.fps_start_time)
            self.fps_frame_count = 0
            self.fps_start_time = current_time

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
        if current_time - self.last_processing_time < self.processing_interval:
            try:
                result, frame_time = self.results_queue.get_nowait()
                return self._apply_processing_result(frame, result, current_time)
            except:
                return frame, 0, 0

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ —Ñ–æ–Ω–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        if self.frame_queue.empty():
            self.frame_queue.put((frame.copy(), current_time))

        self.last_processing_time = current_time

        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        try:
            result, frame_time = self.results_queue.get_nowait()
            return self._apply_processing_result(frame, result, current_time)
        except:
            return frame, 0, 0

    def _apply_processing_result(self, frame, result, current_time):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        processed_frame = frame.copy()
        processed_count = 0

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–∫–∏–Ω–≥
        tracked_faces = self.update_face_tracking(result['faces'], current_time)

        for face_data in tracked_faces:
            visitor_id = self.confirm_visitor_identity(face_data['track_id'], face_data)

            x, y, w, h = face_data['coords']
            status = face_data.get('status', 'detected')

            color = self.get_color_by_status(status)
            status_text = self.get_status_text(status)

            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–∞–º–∫–∏
            cv2.rectangle(processed_frame, (x, y), (x + w, y + h), color, 3)
            cv2.putText(processed_frame, f'{status_text}', (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

            if visitor_id:
                cv2.putText(processed_frame, f'ID: {visitor_id}', (x, y + h + 25),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                cv2.putText(processed_frame, f'Sim: {face_data["similarity"]:.2f}',
                            (x, y + h + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            processed_count += 1

        self.log_recognition_stats()

        return processed_frame, result['detected_count'], processed_count

    def start_analysis(self, rtsp_url):
        """–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –≤–µ—Ä—Å–∏–∏ —Å –û–°–õ–ê–ë–õ–ï–ù–ù–´–ú–ò —Ñ–∏–ª—å—Ç—Ä–∞–º–∏...")

        cap = self.setup_rtsp_camera(rtsp_url)
        if not cap.isOpened():
            return

        self.start_processing_thread()
        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—É—â–µ–Ω —Å –û–°–õ–ê–ë–õ–ï–ù–ù–´–ú–ò —Ñ–∏–ª—å—Ç—Ä–∞–º–∏")

        window_name = 'Trassir Analytics - RELAXED FILTERS'
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(window_name, 1600, 900)

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    logger.warning("üì° –ü–æ—Ç–µ—Ä—è–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∫–∞–º–µ—Ä–æ–π...")
                    time.sleep(2)
                    continue

                processed_frame, detected, processed = self.process_frame_realtime(frame)
                display_frame = self.resize_frame_for_display(processed_frame, target_width=1280)
                display_with_gallery = self.create_gallery_display(display_frame)

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ —ç–∫—Ä–∞–Ω–µ
                stats_text = [
                    f"RELAXED FILTERS - TEST MODE",
                    f"Valid detections: {detected}",
                    f"Visitors processed: {processed}",
                    f"Active tracks: {len(self.face_tracks)}",
                    f"In gallery: {len(self.current_visitors_gallery)}",
                    f"Rejected: {self.recognition_stats['rejected_detections']}",
                    f"FPS: {self.current_fps:.1f}",
                    f"Press 'q' to quit"
                ]

                overlay = display_with_gallery.copy()
                cv2.rectangle(overlay, (0, 0), (550, 220), (0, 0, 0), -1)
                cv2.addWeighted(overlay, 0.7, display_with_gallery, 0.3, 0, display_with_gallery)

                for i, text in enumerate(stats_text):
                    cv2.putText(display_with_gallery, text, (10, 30 + i * 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

                cv2.imshow(window_name, display_with_gallery)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ Ctrl+C...")
        finally:
            self.stop_processing = True
            if self.processing_thread:
                self.processing_thread.join(timeout=2.0)
            cap.release()
            cv2.destroyAllWindows()
            self.conn.close()

            logger.info(f"üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            logger.info(f"   –í—Å–µ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π: {len(self.known_visitors_cache)}")
            logger.info(f"   –ù–æ–≤—ã—Ö —Å–æ–∑–¥–∞–Ω–æ: {self.recognition_stats['new_visitors']}")
            logger.info(f"   –ò–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.recognition_stats['known_visitors']}")
            logger.info(f"   –û—Ç–∫–ª–æ–Ω–µ–Ω–æ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π: {self.recognition_stats['rejected_detections']}")
            if self.recognition_stats['quality_rejections']:
                logger.info(f"   –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π: {dict(self.recognition_stats['quality_rejections'])}")
            logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    RTSP_URL = "rtsp://admin:admin@10.0.0.242:554/live/main"

    counter = ImprovedTrassirCounter(
        processing_interval=1.0,
        similarity_threshold=0.65,
        tracking_threshold=0.50
    )

    try:
        counter.start_analysis(RTSP_URL)
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()