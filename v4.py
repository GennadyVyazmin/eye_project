# video_analytics_trassir_optimized.py
import cv2
import numpy as np
import sqlite3
import datetime
import time
from deepface import DeepFace
import logging
import threading
from queue import Queue
from collections import deque, defaultdict
import os
import shutil
import math
from threading import Lock

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class OptimizedTrassirCounter:
    def __init__(self, processing_interval=2.0, similarity_threshold=0.65, tracking_threshold=0.50):
        """
        –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ CPU
        """
        self.conn = sqlite3.connect('visitors_trassir_optimized.db', check_same_thread=False)
        self._init_database()

        self.processing_interval = processing_interval  # –£–≤–µ–ª–∏—á–∏–ª–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.similarity_threshold = similarity_threshold
        self.tracking_threshold = tracking_threshold

        # –ú—å—é—Ç–µ–∫—Å—ã –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.stats_lock = Lock()
        self.tracks_lock = Lock()
        self.gallery_lock = Lock()

        # –¶–≤–µ—Ç–∞ –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—É—Å–æ–≤
        self.COLORS = {
            'detected': (0, 255, 0),  # –ó–µ–ª–µ–Ω—ã–π - –ª–∏—Ü–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ
            'tracking': (255, 255, 0),  # –ñ–µ–ª—Ç—ã–π - —Å–æ–∑–¥–∞–Ω —Ç—Ä–µ–∫
            'known': (0, 255, 255),  # –ì–æ–ª—É–±–æ–π - –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
            'new': (0, 0, 255),  # –ö—Ä–∞—Å–Ω—ã–π - –Ω–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤ –ë–î
            'analyzing': (255, 165, 0),  # –û—Ä–∞–Ω–∂–µ–≤—ã–π - –∞–Ω–∞–ª–∏–∑ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ
            'rejected': (128, 128, 128)  # –°–µ—Ä—ã–π - –æ—Ç—Å–µ—è–Ω –ª–æ–∂–Ω—ã–π –æ–±—ä–µ–∫—Ç
        }

        # –ü–∞–ø–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ
        self.photos_dir = "visitor_photos_optimized"
        self.current_session_dir = "current_session"
        self._create_directories()

        # –¢—Ä–µ–∫–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.last_processing_time = 0
        self.known_visitors_cache = {}
        self.frame_count = 0

        # –°–∏—Å—Ç–µ–º–∞ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ª–∏—Ü
        self.face_tracks = {}
        self.next_track_id = 1
        self.track_max_age = 8.0

        # –ì–∞–ª–µ—Ä–µ—è —Ç–µ–∫—É—â–∏—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π —Å –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–æ–π
        self.current_visitors_gallery = {}
        self.gallery_max_size = 6  # –£–º–µ–Ω—å—à–∏–ª–∏ —Ä–∞–∑–º–µ—Ä –≥–∞–ª–µ—Ä–µ–∏

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.false_positive_filter = {
            'min_face_width': 50,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            'min_face_height': 50,
            'min_brightness': 20,
            'max_brightness': 240,
            'required_confirmations': 2,  # –¢—Ä–µ–±—É–µ–º –±–æ–ª—å—à–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.recognition_stats = {
            'total_detections': 0,
            'valid_detections': 0,
            'rejected_detections': 0,
            'new_visitors': 0,
            'known_visitors': 0,
            'frames_processed': 0,
            'quality_rejections': defaultdict(int)
        }
        self.last_log_time = time.time()

        # –û—á–µ—Ä–µ–¥—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ - –û–ì–†–ê–ù–ò–ß–ò–õ–ò —Ä–∞–∑–º–µ—Ä
        self.frame_queue = Queue(maxsize=1)  # –í—Å–µ–≥–æ 1 –∫–∞–¥—Ä –≤ –æ—á–µ—Ä–µ–¥–∏
        self.results_queue = Queue()

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Å–∫–∞–¥—ã –æ–¥–∏–Ω —Ä–∞–∑
        logger.info("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ –ª–∏—Ü...")
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )

        self.alt_face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml'
        )

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ö—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_cache = {}
        self.cache_max_size = 50

        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π
        self._load_known_visitors()

        # –ü–æ—Ç–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.processing_thread = None
        self.stop_processing = False

        # –î–ª—è —Ä–∞—Å—á–µ—Ç–∞ FPS
        self.fps_start_time = time.time()
        self.fps_frame_count = 0
        self.current_fps = 0

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°—á–µ—Ç—á–∏–∫ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.frame_skip_counter = 0
        self.frame_skip_interval = 2  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π 3-–π –∫–∞–¥—Ä

        logger.info("üéØ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ"""
        os.makedirs(self.photos_dir, exist_ok=True)
        os.makedirs(os.path.join(self.photos_dir, self.current_session_dir), exist_ok=True)
        logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω—ã –ø–∞–ø–∫–∏ –¥–ª—è —Ñ–æ—Ç–æ: {self.photos_dir}")

    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS visitors (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                face_embedding BLOB,
                first_seen TIMESTAMP,
                last_seen TIMESTAMP,
                visit_count INTEGER DEFAULT 1,
                last_updated TIMESTAMP,
                confirmed_count INTEGER DEFAULT 1,
                photo_path TEXT,
                quality_score REAL DEFAULT 1.0
            )
        ''')
        self.conn.commit()

    def _load_known_visitors(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT id, face_embedding, photo_path FROM visitors")
        visitors = cursor.fetchall()

        self.known_visitors_cache.clear()
        for visitor_id, embedding_blob, photo_path in visitors:
            if embedding_blob:
                try:
                    embedding = np.frombuffer(embedding_blob, dtype=np.float32)
                    self.known_visitors_cache[visitor_id] = embedding
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è {visitor_id}: {e}")

        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π: {len(self.known_visitors_cache)}")

    def setup_rtsp_camera(self, rtsp_url):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ RTSP –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        logger.info(f"üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–∞–º–µ—Ä–µ: {rtsp_url}")
        cap = cv2.VideoCapture(rtsp_url)

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–º–µ—Ä—ã
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        cap.set(cv2.CAP_PROP_FPS, 10)  # –£–º–µ–Ω—å—à–∏–ª–∏ FPS –∫–∞–º–µ—Ä—ã

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ –∫–∞–¥—Ä—ã
        for _ in range(5):
            cap.read()

        if cap.isOpened():
            ret, test_frame = cap.read()
            if ret:
                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                h, w = test_frame.shape[:2]
                if w > 1280:
                    scale = 1280 / w
                    new_w = 1280
                    new_h = int(h * scale)
                    logger.info(f"üìê –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {new_w}x{new_h} (–æ—Ä–∏–≥–∏–Ω–∞–ª: {w}x{h})")

                logger.info(f"‚úÖ –ö–∞–º–µ—Ä–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")
            else:
                logger.error("‚ùå –ö–∞–º–µ—Ä–∞ –Ω–µ –ø–µ—Ä–µ–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∫–∞–º–µ—Ä–µ")

        return cap

    def resize_frame_optimized(self, frame, max_width=1280):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–∞–¥—Ä–∞"""
        h, w = frame.shape[:2]
        if w <= max_width:
            return frame

        scale = max_width / w
        new_w = max_width
        new_h = int(h * scale)

        return cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)

    def analyze_face_quality_fast(self, face_image, bbox):
        """–ë–´–°–¢–†–´–ô –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞"""
        try:
            x, y, w, h = bbox

            # –¢–û–õ–¨–ö–û –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–í–ï–†–ö–ò
            if w < self.false_positive_filter['min_face_width']:
                return False, "small_width"
            if h < self.false_positive_filter['min_face_height']:
                return False, "small_height"

            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —è—Ä–∫–æ—Å—Ç–∏
            gray_face = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
            brightness = np.mean(gray_face)

            if brightness < self.false_positive_filter['min_brightness']:
                return False, "dark"
            if brightness > self.false_positive_filter['max_brightness']:
                return False, "bright"

            return True, "valid"

        except Exception as e:
            return False, f"error: {e}"

    def detect_faces_fast(self, frame):
        """–ë–´–°–¢–†–ê–Ø –¥–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü"""
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
        small_frame = self.resize_frame_optimized(frame, 640)
        gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–∞—Å–∫–∞–¥ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        scale_factor = 1.1
        min_neighbors = 4
        min_size = (50, 50)
        max_size = (300, 300)

        faces = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=scale_factor,
            minNeighbors=min_neighbors,
            minSize=min_size,
            maxSize=max_size,
            flags=cv2.CASCADE_SCALE_IMAGE
        )

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        scale_x = frame.shape[1] / small_frame.shape[1]
        scale_y = frame.shape[0] / small_frame.shape[0]

        scaled_faces = []
        for (x, y, w, h) in faces:
            scaled_faces.append((
                int(x * scale_x),
                int(y * scale_y),
                int(w * scale_x),
                int(h * scale_y)
            ))

        # –ë—ã—Å—Ç—Ä–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        valid_faces = []
        for bbox in scaled_faces:
            x, y, w, h = bbox
            face_roi = frame[y:y + h, x:x + w]

            is_valid, reason = self.analyze_face_quality_fast(face_roi, bbox)
            if is_valid:
                valid_faces.append(bbox)
            else:
                with self.stats_lock:
                    self.recognition_stats['quality_rejections'][reason] += 1
                    self.recognition_stats['rejected_detections'] += 1

        logger.debug(f"üîç –ù–∞–π–¥–µ–Ω–æ –ª–∏—Ü: {len(valid_faces)}")
        return valid_faces

    def get_fast_embedding_optimized(self, face_image):
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ (–ø–æ —Ö—ç—à—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
            img_hash = hash(face_image.tobytes())
            if img_hash in self.embedding_cache:
                return self.embedding_cache[img_hash]

            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            face_resized = cv2.resize(face_image, (160, 160))
            face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)

            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
            result = DeepFace.represent(
                face_rgb,
                model_name='Facenet',  # –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å 'OpenFace' –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                enforce_detection=False,
                detector_backend='skip',  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é, —Ç.–∫. –ª–∏—Ü–æ —É–∂–µ –≤—ã–¥–µ–ª–µ–Ω–æ
                align=False
            )

            embedding = np.array(result[0]['embedding'], dtype=np.float32)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if len(self.embedding_cache) >= self.cache_max_size:
                self.embedding_cache.clear()
            self.embedding_cache[img_hash] = embedding

            return embedding

        except Exception as e:
            logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None

    def calculate_similarity(self, embedding1, embedding2):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        try:
            if embedding1 is None or embedding2 is None:
                return 0.0

            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = np.dot(embedding1, embedding2) / (norm1 * norm2)
            return float(similarity)
        except Exception as e:
            return 0.0

    def find_best_match(self, embedding):
        """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        best_match_id = None
        best_similarity = 0.0

        for visitor_id, known_embedding in self.known_visitors_cache.items():
            similarity = self.calculate_similarity(embedding, known_embedding)

            if similarity > best_similarity and similarity >= self.similarity_threshold:
                best_similarity = similarity
                best_match_id = visitor_id

        return best_match_id, best_similarity

    def update_face_tracking_fast(self, faces, current_time):
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–∏–Ω–≥–∞"""
        active_tracks = {}

        with self.tracks_lock:
            # –ë—ã—Å—Ç—Ä–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ç—Ä–µ–∫–æ–≤
            for track_id, track_info in list(self.face_tracks.items()):
                if current_time - track_info['last_seen'] > self.track_max_age:
                    del self.face_tracks[track_id]

            # –ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–æ–≤
            for face_bbox in faces:
                x, y, w, h = face_bbox
                face_center = (x + w // 2, y + h // 2)

                best_track_id = None
                best_distance = float('inf')

                for track_id, track_info in self.face_tracks.items():
                    if current_time - track_info['last_seen'] > 1.0:
                        continue

                    last_center = track_info['last_center']
                    distance = math.sqrt((face_center[0] - last_center[0]) ** 2 +
                                         (face_center[1] - last_center[1]) ** 2)

                    max_distance = min(w, h) * 1.5

                    if distance < best_distance and distance < max_distance:
                        best_distance = distance
                        best_track_id = track_id

                if best_track_id is not None:
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–∞
                    self.face_tracks[best_track_id].update({
                        'last_seen': current_time,
                        'last_center': face_center,
                        'bbox': face_bbox,
                        'confirmed_count': self.face_tracks[best_track_id].get('confirmed_count', 0) + 1
                    })
                    active_tracks[best_track_id] = self.face_tracks[best_track_id]
                else:
                    # –ù–æ–≤—ã–π —Ç—Ä–µ–∫
                    track_id = self.next_track_id
                    self.next_track_id += 1

                    self.face_tracks[track_id] = {
                        'first_seen': current_time,
                        'last_seen': current_time,
                        'last_center': face_center,
                        'bbox': face_bbox,
                        'confirmed_count': 1,
                        'status': 'detected'
                    }
                    active_tracks[track_id] = self.face_tracks[track_id]

        return active_tracks

    def process_frame_optimized(self, frame):
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞"""
        current_time = time.time()

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–¥—Ä—ã –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
        self.frame_skip_counter += 1
        if self.frame_skip_counter % self.frame_skip_interval != 0:
            return frame, 0, 0

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ FPS
        self.fps_frame_count += 1
        if current_time - self.fps_start_time >= 2.0:  # –†–∞–∑ –≤ 2 —Å–µ–∫—É–Ω–¥—ã
            self.current_fps = self.fps_frame_count / (current_time - self.fps_start_time)
            self.fps_start_time = current_time
            self.fps_frame_count = 0

        # –ë–´–°–¢–†–ê–Ø –¥–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü
        faces = self.detect_faces_fast(frame)

        # –ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–∏–Ω–≥–∞
        active_tracks = self.update_face_tracking_fast(faces, current_time)

        # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        processed_frame = frame.copy()
        detected_count = 0
        processed_count = 0

        for track_id, track_info in active_tracks.items():
            x, y, w, h = track_info['bbox']

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
            if track_info.get('visitor_id'):
                status = 'known' if track_info.get('status') == 'known' else 'new'
            elif track_info['confirmed_count'] >= self.false_positive_filter['required_confirmations']:
                status = 'tracking'
            else:
                status = 'detected'

            color = self.COLORS[status]

            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞
            cv2.rectangle(processed_frame, (x, y), (x + w, y + h), color, 2)
            label = f"ID:{track_id}"
            cv2.putText(processed_frame, label, (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            detected_count += 1

            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤
            if (track_info['confirmed_count'] >= 3 and
                    current_time - track_info.get('last_processed', 0) > self.processing_interval):

                try:
                    face_roi = frame[y:y + h, x:x + w]
                    embedding = self.get_fast_embedding_optimized(face_roi)

                    if embedding is not None:
                        visitor_id, similarity = self.find_best_match(embedding)
                        if visitor_id:
                            track_info['visitor_id'] = visitor_id
                            track_info['status'] = 'known'
                            processed_count += 1
                            logger.info(f"üë§ –†–∞—Å–ø–æ–∑–Ω–∞–Ω –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—å {visitor_id} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.3f})")

                    track_info['last_processed'] = current_time

                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏—Ü–∞: {e}")

        with self.stats_lock:
            self.recognition_stats['total_detections'] += len(faces)
            self.recognition_stats['valid_detections'] += detected_count
            self.recognition_stats['frames_processed'] += 1

        return processed_frame, detected_count, processed_count

    def start_analysis_optimized(self, rtsp_url):
        """–ó–∞–ø—É—Å–∫ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –∞–Ω–∞–ª–∏–∑–∞"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –≤–µ—Ä—Å–∏–∏...")

        cap = self.setup_rtsp_camera(rtsp_url)
        if not cap.isOpened():
            return

        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—É—â–µ–Ω –≤ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ú —Ä–µ–∂–∏–º–µ!")

        window_name = 'Trassir Analytics - OPTIMIZED'
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)

        try:
            while True:
                start_time = time.time()

                ret, frame = cap.read()
                if not ret:
                    logger.warning("üì° –ü–æ—Ç–µ—Ä—è–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ...")
                    time.sleep(1)
                    continue

                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                processed_frame, detected, processed = self.process_frame_optimized(frame)

                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                display_frame = self.resize_frame_optimized(processed_frame, 1280)

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ —ç–∫—Ä–∞–Ω–µ
                stats_text = [
                    f"OPTIMIZED MODE - CPU SAVER",
                    f"FPS: {self.current_fps:.1f}",
                    f"Detected: {detected}",
                    f"Tracks: {len(self.face_tracks)}",
                    f"Frame skip: {self.frame_skip_interval}",
                    f"Press 'q' to quit"
                ]

                # –ü—Ä–æ—Å—Ç–æ–π –æ–≤–µ—Ä–ª–µ–π
                for i, text in enumerate(stats_text):
                    cv2.putText(display_frame, text, (10, 30 + i * 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 3)
                    cv2.putText(display_frame, text, (10, 30 + i * 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

                cv2.imshow(window_name, display_frame)

                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
                processing_time = time.time() - start_time
                delay = max(1, int(30 - processing_time * 1000))  # –¶–µ–ª–µ–≤–æ–π FPS ~30

                if cv2.waitKey(delay) & 0xFF == ord('q'):
                    break

        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞...")
        finally:
            self.stop_processing = True
            cap.release()
            cv2.destroyAllWindows()
            self.conn.close()

            logger.info(f"üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–¥—Ä–æ–≤: {self.recognition_stats['frames_processed']}")
            logger.info(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ª–∏—Ü: {self.recognition_stats['valid_detections']}")
            logger.info(f"   –°—Ä–µ–¥–Ω–∏–π FPS: {self.current_fps:.1f}")
            logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    RTSP_URL = "rtsp://admin:admin@10.0.0.242:554/live/main"

    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
    counter = OptimizedTrassirCounter(
        processing_interval=3.0,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏
        similarity_threshold=0.60,
        tracking_threshold=0.45
    )

    try:
        counter.start_analysis_optimized(RTSP_URL)
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()