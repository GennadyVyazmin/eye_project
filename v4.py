# video_analytics_trassir_lightweight.py
import cv2
import numpy as np
import sqlite3
import datetime
import time
import logging
import threading
from queue import Queue
import os
import mediapipe as mp

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('face_detection.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class LightweightTrassirCounter:
    def __init__(self, processing_interval=1.0, tracking_threshold=0.6):
        """
        –û–±–ª–µ–≥—á–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ DeepFace, —Ç–æ–ª—å–∫–æ MediaPipe + OpenCV
        """
        self.conn = sqlite3.connect('visitors_trassir_lightweight.db', check_same_thread=False)
        self._init_database()

        self.processing_interval = processing_interval
        self.tracking_threshold = tracking_threshold

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.min_face_size = 80
        self.max_face_size = 400
        self.min_confidence = 0.6

        # –¶–≤–µ—Ç–∞ –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—É—Å–æ–≤
        self.COLORS = {
            'detected': (0, 255, 0),  # –ó–µ–ª–µ–Ω—ã–π - –ª–∏—Ü–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ
            'tracking': (255, 255, 0),  # –ñ–µ–ª—Ç—ã–π - —Å–æ–∑–¥–∞–Ω —Ç—Ä–µ–∫
            'known': (0, 255, 255),  # –ì–æ–ª—É–±–æ–π - –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
            'new': (0, 0, 255),  # –ö—Ä–∞—Å–Ω—ã–π - –Ω–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤ –ë–î
            'analyzing': (255, 165, 0),  # –û—Ä–∞–Ω–∂–µ–≤—ã–π - –∞–Ω–∞–ª–∏–∑ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ
            'filtered': (128, 128, 128)  # –°–µ—Ä—ã–π - –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ
        }

        # –ü–∞–ø–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ
        self.photos_dir = "visitor_photos_lightweight"
        self.current_session_dir = "current_session"
        self._create_directories()

        # –¢—Ä–µ–∫–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.last_processing_time = 0
        self.known_visitors = {}  # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ª–∏—Ü
        self.next_visitor_id = 1

        # –°–∏—Å—Ç–µ–º–∞ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ª–∏—Ü
        self.face_tracks = {}
        self.next_track_id = 1
        self.track_max_age = 5.0

        # –ì–∞–ª–µ—Ä–µ—è —Ç–µ–∫—É—â–∏—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π
        self.current_visitors_gallery = {}
        self.gallery_max_size = 8
        self.photo_size = (120, 160)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.recognition_stats = {
            'total_detections': 0,
            'new_visitors': 0,
            'known_visitors': 0,
            'frames_processed': 0,
            'filtered_detections': 0
        }
        self.last_log_time = time.time()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe
        self.setup_mediapipe()

        # –î–ª—è —Ä–∞—Å—á–µ—Ç–∞ FPS
        self.fps_start_time = time.time()
        self.fps_frame_count = 0
        self.current_fps = 0

        logger.info("üéØ –û–±–ª–µ–≥—á–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (—Ç–æ–ª—å–∫–æ MediaPipe)")

    def setup_mediapipe(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe"""
        try:
            self.mp_face_detection = mp.solutions.face_detection
            self.mp_drawing = mp.solutions.drawing_utils
            self.face_detection = self.mp_face_detection.FaceDetection(
                model_selection=0,  # 0 –¥–ª—è –±–ª–∏–∂–Ω–∏—Ö, 1 –¥–ª—è –¥–∞–ª—å–Ω–∏—Ö –ª–∏—Ü
                min_detection_confidence=0.5
            )
            logger.info("‚úÖ MediaPipe –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ MediaPipe: {e}")
            raise

    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ"""
        os.makedirs(self.photos_dir, exist_ok=True)
        os.makedirs(os.path.join(self.photos_dir, self.current_session_dir), exist_ok=True)
        logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω—ã –ø–∞–ø–∫–∏ –¥–ª—è —Ñ–æ—Ç–æ: {self.photos_dir}")

    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS visitors (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                first_seen TIMESTAMP,
                last_seen TIMESTAMP,
                visit_count INTEGER DEFAULT 1,
                last_updated TIMESTAMP,
                photo_path TEXT,
                track_features BLOB
            )
        ''')
        self.conn.commit()

    def detect_faces_mediapipe(self, frame):
        """–î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MediaPipe"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_detection.process(rgb_frame)

        faces = []
        if results.detections:
            for detection in results.detections:
                bbox = detection.location_data.relative_bounding_box
                h, w = frame.shape[:2]

                x = int(bbox.xmin * w)
                y = int(bbox.ymin * h)
                width = int(bbox.width * w)
                height = int(bbox.height * h)

                confidence = detection.score[0]

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–º–µ—Ä—É
                if (confidence >= self.min_confidence and
                        width >= self.min_face_size and height >= self.min_face_size and
                        width <= self.max_face_size and height <= self.max_face_size):

                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
                    if self.is_valid_face_region(frame, x, y, width, height):
                        faces.append({
                            'coords': (x, y, width, height),
                            'confidence': confidence,
                            'keypoints': detection.location_data.relative_keypoints
                        })

        return faces

    def is_valid_face_region(self, frame, x, y, w, h):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ä–µ–≥–∏–æ–Ω–∞ –ª–∏—Ü–∞"""
        h_total, w_total = frame.shape[:2]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Ö–æ–¥–∞ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã
        if x < 0 or y < 0 or x + w > w_total or y + h > h_total:
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ –∫–∞–¥—Ä–∞)
        if w < w_total * 0.05 or h < h_total * 0.05:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π
            return False
        if w > w_total * 0.4 or h > h_total * 0.4:  # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
            return False

        return True

    def validate_human_features(self, face_image):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –∏–º–µ–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —á–µ–ª–æ–≤–µ–∫–∞"""
        if face_image.size == 0:
            return False

        h, w = face_image.shape[:2]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        if w < 50 or h < 50 or w > 400 or h > 400:
            return False

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–≤–µ—Ç–æ–≤–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–∫–æ–∂–∞ —á–µ–ª–æ–≤–µ–∫–∞)
            hsv = cv2.cvtColor(face_image, cv2.COLOR_BGR2HSV)

            # –ú–∞—Å–∫–∞ –¥–ª—è —Ü–≤–µ—Ç–æ–≤ –∫–æ–∂–∏
            skin_lower = np.array([0, 20, 70], dtype=np.uint8)
            skin_upper = np.array([20, 255, 255], dtype=np.uint8)
            skin_mask = cv2.inRange(hsv, skin_lower, skin_upper)

            # –ü—Ä–æ—Ü–µ–Ω—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫–æ–∂–∏
            skin_ratio = np.sum(skin_mask > 0) / (w * h)

            # –î–ª—è –ª–∏—Ü –æ–±—ã—á–Ω–æ 15-50% –ø–∏–∫—Å–µ–ª–µ–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ü–≤–µ—Ç—É –∫–æ–∂–∏
            return 0.1 < skin_ratio < 0.7
        except:
            return True  # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–∞–µ–º —à–∞–Ω—Å

    def extract_simple_features(self, face_image):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ –±–µ–∑ DeepFace"""
        try:
            # –†–µ—Å–∞–π–∑ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
            resized = cv2.resize(face_image, (64, 64))

            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ grayscale
            gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)

            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0)
            gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1)

            mag, ang = cv2.cartToPolar(gx, gy)

            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: —Å—Ä–µ–¥–Ω—è—è —è—Ä–∫–æ—Å—Ç—å, –∫–æ–Ω—Ç—Ä–∞—Å—Ç, –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
            features = []
            features.append(np.mean(gray))  # –°—Ä–µ–¥–Ω—è—è —è—Ä–∫–æ—Å—Ç—å
            features.append(np.std(gray))  # –ö–æ–Ω—Ç—Ä–∞—Å—Ç
            features.extend(np.histogram(gray, bins=8)[0])  # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            features = np.array(features, dtype=np.float32)
            if np.linalg.norm(features) > 0:
                features = features / np.linalg.norm(features)

            return features
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return None

    def calculate_feature_similarity(self, features1, features2):
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if features1 is None or features2 is None:
            return 0.0

        try:
            # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = np.dot(features1, features2)
            return max(0.0, min(1.0, similarity))
        except:
            return 0.0

    def update_face_tracking(self, current_faces, timestamp):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–∏–Ω–≥–∞ –ª–∏—Ü –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏"""
        updated_faces = []

        for face_data in current_faces:
            features = face_data['features']
            coords = face_data['coords']
            face_image = face_data['face_image']

            best_track_id = None
            best_similarity = 0.0

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ç—Ä–µ–∫–∏
            for track_id in list(self.face_tracks.keys()):
                if timestamp - self.face_tracks[track_id]['last_seen'] > self.track_max_age:
                    logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ç—Ä–µ–∫ {track_id}")
                    del self.face_tracks[track_id]

            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ç—Ä–µ–∫–∞–º–∏
            for track_id, track_data in self.face_tracks.items():
                similarity = self.calculate_feature_similarity(features, track_data['features'])
                if similarity > best_similarity and similarity > self.tracking_threshold:
                    best_similarity = similarity
                    best_track_id = track_id

            if best_track_id is not None:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–µ–∫
                self.face_tracks[best_track_id].update({
                    'features': features,
                    'last_seen': timestamp,
                    'coords': coords,
                    'face_image': face_image
                })
                face_data['track_id'] = best_track_id
                face_data['visitor_id'] = self.face_tracks[best_track_id].get('visitor_id')
                face_data['status'] = 'tracking'
                face_data['similarity'] = best_similarity
                logger.debug(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω —Ç—Ä–µ–∫ {best_track_id}, —Å—Ö–æ–∂–µ—Å—Ç—å: {best_similarity:.3f}")
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç—Ä–µ–∫
                track_id = self.next_track_id
                self.next_track_id += 1
                self.face_tracks[track_id] = {
                    'features': features,
                    'last_seen': timestamp,
                    'coords': coords,
                    'face_image': face_image,
                    'visitor_id': None,
                    'created_at': timestamp
                }
                face_data['track_id'] = track_id
                face_data['status'] = 'tracking'
                face_data['similarity'] = 0.0
                logger.info(f"üéØ –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —Ç—Ä–µ–∫ {track_id}")

            updated_faces.append(face_data)

        return updated_faces

    def confirm_visitor_identity(self, track_id, face_data):
        """–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è"""
        track_data = self.face_tracks.get(track_id)
        if not track_data:
            return None

        features = face_data['features']
        face_image = face_data['face_image']
        timestamp = time.time()

        # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å visitor_id, –æ–±–Ω–æ–≤–ª—è–µ–º –µ–≥–æ
        if track_data['visitor_id']:
            visitor_id = track_data['visitor_id']
            self.recognition_stats['known_visitors'] += 1
            face_data['status'] = 'known'

            # –û–±–Ω–æ–≤–ª—è–µ–º –≥–∞–ª–µ—Ä–µ—é
            self.update_visitor_gallery(visitor_id, face_image, 'known')

            logger.debug(f"‚ôªÔ∏è  –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—å {visitor_id}")
            return visitor_id

        # –ò—â–µ–º –ª—É—á—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å—Ä–µ–¥–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö
        visitor_id, similarity = self.find_best_match(features)

        if similarity > 0.7:  # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è
            track_data['visitor_id'] = visitor_id
            track_data['confirmed_at'] = timestamp
            self.recognition_stats['known_visitors'] += 1
            face_data['status'] = 'known'

            # –û–±–Ω–æ–≤–ª—è–µ–º –≥–∞–ª–µ—Ä–µ—é
            self.update_visitor_gallery(visitor_id, face_image, 'known')

            logger.info(f"üë§ –û–ü–û–ó–ù–ê–ù –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—å {visitor_id}, —Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f}")
            return visitor_id
        else:
            # –ñ–¥–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è
            track_duration = timestamp - track_data['created_at']
            if track_duration > 3.0:  # –£–≤–µ–ª–∏—á–∏–ª–∏ –≤—Ä–µ–º—è –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞
                new_visitor_id = self._create_new_visitor(features, face_image, track_id)
                if new_visitor_id:
                    self.recognition_stats['new_visitors'] += 1
                    face_data['status'] = 'new'

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–æ—Ç–æ –∏ –æ–±–Ω–æ–≤–ª—è–µ–º –≥–∞–ª–µ—Ä–µ—é
                    self.update_visitor_gallery(new_visitor_id, face_image, 'new')

                    logger.info(f"üÜï –°–û–ó–î–ê–ù –Ω–æ–≤—ã–π –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—å {new_visitor_id}")
                return new_visitor_id
            else:
                face_data['status'] = 'analyzing'
                logger.debug(f"‚è≥ –¢—Ä–µ–∫ {track_id} –æ–∂–∏–¥–∞–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è ({track_duration:.1f}s)")

        return None

    def find_best_match(self, features):
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ä–µ–¥–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π"""
        if features is None:
            return None, 0.0

        best_match_id = None
        best_similarity = 0.0

        for visitor_id, visitor_data in self.known_visitors.items():
            similarity = self.calculate_feature_similarity(features, visitor_data['features'])
            if similarity > best_similarity:
                best_similarity = similarity
                best_match_id = visitor_id

        return best_match_id, best_similarity

    def _create_new_visitor(self, features, face_image, track_id):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è"""
        cursor = self.conn.cursor()
        now = datetime.datetime.now()

        visitor_id = None
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–æ—Ç–æ
            photo_path = self.save_visitor_photo(face_image, "temp")

            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –±–∞–∑–µ
            features_blob = features.astype(np.float32).tobytes()
            cursor.execute(
                """INSERT INTO visitors (first_seen, last_seen, visit_count, 
                   last_updated, photo_path, track_features) 
                   VALUES (?, ?, 1, ?, ?, ?)""",
                (now, now, now, photo_path, features_blob)
            )
            visitor_id = cursor.lastrowid

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –∫ —Ñ–æ—Ç–æ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º ID
            final_photo_path = self.save_visitor_photo(face_image, visitor_id)
            cursor.execute(
                "UPDATE visitors SET photo_path = ? WHERE id = ?",
                (final_photo_path, visitor_id)
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç–∏
            self.known_visitors[visitor_id] = {
                'features': features,
                'photo_path': final_photo_path,
                'first_seen': now
            }

            self.conn.commit()

            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ñ–æ—Ç–æ
            if os.path.exists(photo_path):
                os.remove(photo_path)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è: {e}")
            self.conn.rollback()
            return None

        if track_id in self.face_tracks:
            self.face_tracks[track_id]['visitor_id'] = visitor_id

        return visitor_id

    def save_visitor_photo(self, face_image, visitor_id):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–æ—Ç–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è"""
        try:
            photo_clean = face_image.copy()

            height, width = photo_clean.shape[:2]
            if width < 200:
                scale = 200 / width
                new_width = 200
                new_height = int(height * scale)
                photo_clean = cv2.resize(photo_clean, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)

            filename = f"visitor_{visitor_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
            filepath = os.path.join(self.photos_dir, filename)
            session_filepath = os.path.join(self.photos_dir, self.current_session_dir, filename)

            cv2.imwrite(filepath, photo_clean)
            cv2.imwrite(session_filepath, photo_clean)

            logger.info(f"üì∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–æ—Ç–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è {visitor_id}")
            return filepath

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–æ—Ç–æ: {e}")
            return ""

    def update_visitor_gallery(self, visitor_id, face_image, status):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥–∞–ª–µ—Ä–µ–∏ —Ç–µ–∫—É—â–∏—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π"""
        try:
            gallery_photo = face_image.copy()

            border_color = self.COLORS.get(status, (255, 255, 255))
            gallery_photo = cv2.copyMakeBorder(
                gallery_photo, 5, 25, 5, 5, cv2.BORDER_CONSTANT, value=border_color
            )

            status_text = self.get_status_text(status)
            cv2.putText(gallery_photo, f"ID: {visitor_id}", (10, gallery_photo.shape[0] - 15),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, border_color, 1)

            gallery_photo = cv2.resize(gallery_photo, self.photo_size, interpolation=cv2.INTER_AREA)

            self.current_visitors_gallery[visitor_id] = {
                'photo': gallery_photo,
                'last_seen': time.time(),
                'status': status
            }

            if len(self.current_visitors_gallery) > self.gallery_max_size:
                oldest_visitor = min(self.current_visitors_gallery.keys(),
                                     key=lambda x: self.current_visitors_gallery[x]['last_seen'])
                del self.current_visitors_gallery[oldest_visitor]

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–∞–ª–µ—Ä–µ–∏: {e}")

    def get_status_text(self, status):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ç–∞—Ç—É—Å—É"""
        status_texts = {
            'detected': 'DETECTED',
            'tracking': 'TRACKING',
            'analyzing': 'ANALYZING',
            'known': 'KNOWN',
            'new': 'NEW USER',
            'filtered': 'FILTERED'
        }
        return status_texts.get(status, 'UNKNOWN')

    def get_color_by_status(self, status):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –ø–æ —Å—Ç–∞—Ç—É—Å—É"""
        return self.COLORS.get(status, (255, 255, 255))

    def create_gallery_display(self, main_frame):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥–∞–ª–µ—Ä–µ–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π"""
        try:
            main_height, main_width = main_frame.shape[:2]

            gallery_width = 300
            gallery_panel = np.zeros((main_height, gallery_width, 3), dtype=np.uint8)

            cv2.putText(gallery_panel, "CURRENT VISITORS", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(gallery_panel, f"Total: {len(self.current_visitors_gallery)}", (10, 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            if self.current_visitors_gallery:
                visitor_ids = sorted(self.current_visitors_gallery.keys())
                photos_per_column = 4
                photo_width, photo_height = self.photo_size
                margin = 10

                for i, visitor_id in enumerate(visitor_ids):
                    if i >= self.gallery_max_size:
                        break

                    visitor_data = self.current_visitors_gallery[visitor_id]
                    row = i % photos_per_column
                    col = i // photos_per_column

                    x = margin + col * (photo_width + margin)
                    y = 80 + row * (photo_height + margin)

                    if y + photo_height < main_height and x + photo_width < gallery_width:
                        gallery_panel[y:y + photo_height, x:x + photo_width] = visitor_data['photo']

                        status_color = self.COLORS.get(visitor_data['status'], (255, 255, 255))
                        cv2.circle(gallery_panel, (x + 10, y + 10), 5, status_color, -1)
            else:
                cv2.putText(gallery_panel, "No visitors", (50, main_height // 2),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 1)
                cv2.putText(gallery_panel, "in frame", (60, main_height // 2 + 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 1)

            combined_frame = np.hstack([main_frame, gallery_panel])
            return combined_frame

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥–∞–ª–µ—Ä–µ–∏: {e}")
            return main_frame

    def resize_frame_for_display(self, frame, target_width=1280):
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–∞–¥—Ä–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        height, width = frame.shape[:2]

        if width <= target_width:
            return frame

        ratio = target_width / width
        new_width = target_width
        new_height = int(height * ratio)

        resized_frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_LINEAR)

        return resized_frame

    def log_recognition_stats(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è"""
        current_time = time.time()
        if current_time - self.last_log_time >= 3.0:
            logger.info(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê: –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(self.known_visitors)}, "
                        f"–ê–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤: {len(self.face_tracks)}, "
                        f"–í –≥–∞–ª–µ—Ä–µ–µ: {len(self.current_visitors_gallery)}, "
                        f"–ù–æ–≤—ã—Ö –∑–∞ —Å–µ—Å—Å–∏—é: {self.recognition_stats['new_visitors']}, "
                        f"–ò–∑–≤–µ—Å—Ç–Ω—ã—Ö: {self.recognition_stats['known_visitors']}, "
                        f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {self.recognition_stats['filtered_detections']}")
            self.last_log_time = current_time

    def process_frame(self, frame):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
        current_time = time.time()

        # –û–±–Ω–æ–≤–ª—è–µ–º FPS
        self.fps_frame_count += 1
        if current_time - self.fps_start_time >= 1.0:
            self.current_fps = self.fps_frame_count / (current_time - self.fps_start_time)
            self.fps_frame_count = 0
            self.fps_start_time = current_time

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
        if current_time - self.last_processing_time < self.processing_interval:
            return frame, 0, 0, 0

        # –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü
        detected_faces = self.detect_faces_mediapipe(frame)

        processed_faces = []
        filtered_count = 0

        if detected_faces:
            self.recognition_stats['total_detections'] += len(detected_faces)
            logger.info(f"üë• –û–ë–ù–ê–†–£–ñ–ï–ù–û –û–ë–™–ï–ö–¢–û–í: {len(detected_faces)}")

            for face_data in detected_faces:
                x, y, w, h = face_data['coords']

                face_img = frame[y:y + h, x:x + w]

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —á–µ–ª–æ–≤–µ–∫–∞
                if not self.validate_human_features(face_img):
                    filtered_count += 1
                    logger.debug("‚ùå –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –Ω–µ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç")
                    continue

                features = self.extract_simple_features(face_img)
                if features is not None:
                    processed_faces.append({
                        'coords': (x, y, w, h),
                        'features': features,
                        'confidence': face_data['confidence'],
                        'face_image': face_img,
                        'status': 'detected'
                    })

        self.recognition_stats['filtered_detections'] += filtered_count

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–∫–∏–Ω–≥
        tracked_faces = self.update_face_tracking(processed_faces, current_time)

        # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        processed_frame = frame.copy()
        processed_count = 0

        for face_data in tracked_faces:
            visitor_id = self.confirm_visitor_identity(face_data['track_id'], face_data)

            x, y, w, h = face_data['coords']
            status = face_data.get('status', 'detected')

            color = self.get_color_by_status(status)
            status_text = self.get_status_text(status)

            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–∞–º–∫–∏
            cv2.rectangle(processed_frame, (x, y), (x + w, y + h), color, 3)
            cv2.putText(processed_frame, f'{status_text}', (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

            if visitor_id:
                cv2.putText(processed_frame, f'ID: {visitor_id}', (x, y + h + 25),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                if 'similarity' in face_data:
                    cv2.putText(processed_frame, f'Sim: {face_data["similarity"]:.2f}',
                                (x, y + h + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            processed_count += 1

        self.last_processing_time = current_time
        self.log_recognition_stats()

        return processed_frame, len(detected_faces), processed_count, filtered_count

    def setup_rtsp_camera(self, rtsp_url):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ RTSP"""
        logger.info(f"üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–∞–º–µ—Ä–µ: {rtsp_url}")
        cap = cv2.VideoCapture(rtsp_url)

        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        cap.set(cv2.CAP_PROP_FPS, 15)
        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'H264'))

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–¥—Ä—ã –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
        for _ in range(10):
            cap.read()

        if cap.isOpened():
            ret, test_frame = cap.read()
            if ret:
                logger.info(f"‚úÖ –ö–∞–º–µ—Ä–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞. –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {test_frame.shape[1]}x{test_frame.shape[0]}")
            else:
                logger.error("‚ùå –ö–∞–º–µ—Ä–∞ –Ω–µ –ø–µ—Ä–µ–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∫–∞–º–µ—Ä–µ")

        return cap

    def start_analysis(self, rtsp_url):
        """–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±–ª–µ–≥—á–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ —Å MediaPipe...")

        cap = self.setup_rtsp_camera(rtsp_url)
        if not cap.isOpened():
            return

        logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—É—â–µ–Ω")

        window_name = 'Trassir Analytics - LIGHTWEIGHT (MediaPipe only)'
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(window_name, 1600, 900)

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    logger.warning("üì° –ü–æ—Ç–µ—Ä—è–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∫–∞–º–µ—Ä–æ–π...")
                    time.sleep(2)
                    continue

                processed_frame, detected, processed, filtered = self.process_frame(frame)
                display_frame = self.resize_frame_for_display(processed_frame, target_width=1280)
                display_with_gallery = self.create_gallery_display(display_frame)

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ —ç–∫—Ä–∞–Ω–µ
                stats_text = [
                    f"LIGHTWEIGHT ANALYTICS (MediaPipe)",
                    f"Objects detected: {detected}",
                    f"Faces processed: {processed}",
                    f"Filtered: {filtered}",
                    f"Active tracks: {len(self.face_tracks)}",
                    f"In gallery: {len(self.current_visitors_gallery)}",
                    f"FPS: {self.current_fps:.1f}",
                    f"Press 'q' to quit"
                ]

                overlay = display_with_gallery.copy()
                cv2.rectangle(overlay, (0, 0), (500, 220), (0, 0, 0), -1)
                cv2.addWeighted(overlay, 0.7, display_with_gallery, 0.3, 0, display_with_gallery)

                for i, text in enumerate(stats_text):
                    color = (255, 255, 255)
                    if "Filtered" in text and filtered > 0:
                        color = (0, 255, 255)  # –ñ–µ–ª—Ç—ã–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
                    cv2.putText(display_with_gallery, text, (10, 30 + i * 25),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

                cv2.imshow(window_name, display_with_gallery)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ Ctrl+C...")
        finally:
            cap.release()
            cv2.destroyAllWindows()
            self.conn.close()

            logger.info(f"üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            logger.info(f"   –í—Å–µ–≥–æ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π: {len(self.known_visitors)}")
            logger.info(f"   –ù–æ–≤—ã—Ö —Å–æ–∑–¥–∞–Ω–æ: {self.recognition_stats['new_visitors']}")
            logger.info(f"   –ò–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.recognition_stats['known_visitors']}")
            logger.info(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {self.recognition_stats['filtered_detections']}")
            logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    RTSP_URL = "rtsp://admin:admin@10.0.0.242:554/live/main"

    counter = LightweightTrassirCounter(
        processing_interval=1.0,
        tracking_threshold=0.6
    )

    try:
        counter.start_analysis(RTSP_URL)
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()